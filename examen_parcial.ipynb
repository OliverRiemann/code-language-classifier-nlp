{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exámen parcial Bourbaki track\n",
    "\n",
    "En [este sitio](https://developer.ibm.com/exchanges/data/all/project-codenet/) considere el dataset llamado Project_CodeNet_LangClass.tgz el cual consiste en un conjunto de códigos en 10 lenguajes de programación distintos. Además es posible construir una etiqueta que corresponda al lenguaje de programación al que pertenece.\n",
    "\n",
    "Se deberá de construir una red neuronal para clasificar estos códigos de acuerdo al lenguaje de programación correspondiente. Idealmente sugerimos utilizar una red neuronal recurrente aunque es posible utilizar otros acercamientos. Las funciones de activación y de pérdida sugeridas son Softmax y Cross-Entropy respectivamente.\n",
    "\n",
    "Es importante notar que el proceso de tokenización no es de ninguna manera obvio, se podría trabajar con una bolsa de palabras hecha\n",
    "únicamente con caracteres.\n",
    "\n",
    "También se podría entrenar un encaje al estilo de los autoencoders que estudiamos, en el sitio donde se puede descargar el dataset es posible descargar un conjunto de datos más grande con el que se podría entrenar un\n",
    "tal encaje. Hacer esto no es indispensable pues es más complicado. \n",
    "\n",
    "Pueden trabajar en local o en Google Colab, al finalizar deberán enviar el archivo o el link con permisos para comentar a Braulio, Édison Vázquez y Alfonso Ruiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero voy a leer cada archivo línea por línea. Para eso definiré la siguiente función read_code_files, la cual genera una lista de tuplets con el código como texto y el típo de archivo (c, py, etc.). Nótese que la lectura del texto respeta el salto de línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def read_code_files(folder_path):\n",
    "    file_paths = list(Path(folder_path).rglob(\"*.*\"))\n",
    "    data = []\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "            label = path.suffix[1:]  # 'py', 'c', etc.\n",
    "            data.append((content, label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_code_files(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después convierto esta lista en un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>import Control.Monad\\nimport Control.Applicati...</td>\n",
       "      <td>hs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>\\n//Digit Number\\nimport java.io.BufferedReade...</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>###\\n### atcorder test program\\n###\\n\\nimport ...</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>//using System;\\n//class p0003\\n//{\\n//    sta...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>public class Main {\\n\\n    public static void ...</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>using System;\\nusing System.IO;\\nusing System....</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>#include &lt;bits/stdc++.h&gt;\\nusing namespace std;...</td>\n",
       "      <td>cpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>import java.io.*;\\n\\nclass Main {\\n\\tpublic st...</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>&lt;?php\\n\\nwhile (($line = trim(fgets(STDIN))) !...</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td># coding=utf-8\\n\\n\\ndef direction_vector(p1: l...</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  code label\n",
       "590  import Control.Monad\\nimport Control.Applicati...    hs\n",
       "534  \\n//Digit Number\\nimport java.io.BufferedReade...  java\n",
       "211  ###\\n### atcorder test program\\n###\\n\\nimport ...    py\n",
       "29   //using System;\\n//class p0003\\n//{\\n//    sta...    cs\n",
       "513  public class Main {\\n\\n    public static void ...  java\n",
       "353  using System;\\nusing System.IO;\\nusing System....    cs\n",
       "103  #include <bits/stdc++.h>\\nusing namespace std;...   cpp\n",
       "523  import java.io.*;\\n\\nclass Main {\\n\\tpublic st...  java\n",
       "656  <?php\\n\\nwhile (($line = trim(fgets(STDIN))) !...   php\n",
       "214  # coding=utf-8\\n\\n\\ndef direction_vector(p1: l...    py"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df, columns=[\"code\", \"label\"])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio voy a tokenizar por palabras o símbolos individuales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(code):\n",
    "    return re.findall(r'\\w+|[^\\s\\w]', code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " 'Bud1',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x10',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x08',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x10',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " '\\x08',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x08',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x08',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x02',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x02',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x10',\n",
       " '\\x00',\n",
       " 'bwspblob',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x02',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x04',\n",
       " '\\x00',\n",
       " 't',\n",
       " '\\x00',\n",
       " 'e',\n",
       " '\\x00',\n",
       " 's',\n",
       " '\\x00',\n",
       " 'tbwspblob',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " 'bplist00',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x03',\n",
       " '\\x04',\n",
       " '\\x05',\n",
       " '\\x06',\n",
       " '\\x07',\n",
       " '\\x08',\n",
       " '\\x07',\n",
       " '\\x08',\n",
       " '\\x08',\n",
       " ']',\n",
       " 'ShowStatusBar',\n",
       " '[',\n",
       " 'ShowToolbar',\n",
       " '[',\n",
       " 'ShowTabView_',\n",
       " '\\x10',\n",
       " '\\x14',\n",
       " 'ContainerShowSidebar',\n",
       " '\\\\',\n",
       " 'WindowBounds',\n",
       " '[',\n",
       " 'ShowSidebar',\n",
       " '\\x08',\n",
       " '\\x08',\n",
       " '_',\n",
       " '\\x10',\n",
       " '\\x16',\n",
       " '{',\n",
       " '{',\n",
       " '29',\n",
       " ',',\n",
       " '79',\n",
       " '}',\n",
       " ',',\n",
       " '{',\n",
       " '868',\n",
       " ',',\n",
       " '796',\n",
       " '}',\n",
       " '}',\n",
       " '\\x08',\n",
       " '\\x15',\n",
       " '#',\n",
       " '/',\n",
       " ';',\n",
       " 'R_klmno',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " '\\x01',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x04',\n",
       " '\\x00',\n",
       " 't',\n",
       " '\\x00',\n",
       " 'e',\n",
       " '\\x00',\n",
       " 's',\n",
       " '\\x00',\n",
       " 'tvSrnlong',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " '\\x00',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(df[\"code\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenizamos todos los códigos\n",
    "all_tokens = []\n",
    "for code in df['code']:\n",
    "    tokens = tokenize(code)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Construimos vocabulario\n",
    "token_freqs = Counter(all_tokens)\n",
    "vocab = {token: i+2 for i, (token, _) in enumerate(token_freqs.items())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapeamos tokens a índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ids(tokens, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos el tensor con el que entrenaremos nuestra RNN.\n",
    "\n",
    "Primero codificamos nuestras etiquetas con LabelEncoder, después tokenizamos nuestos textos y lo convertimos a índices, después hacemos padding a las secuencias, convertimos estas secuencias a un tensor y finalmente convertimos a tensor estas secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1002, 500])\n",
      "torch.Size([1002])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Codificamos etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "# Secuencias tokenizadas\n",
    "MAX_LEN = 500\n",
    "encoded_sequences = []\n",
    "\n",
    "for code in df[\"code\"]:\n",
    "    tokens = tokenize(code)\n",
    "    token_ids = tokens_to_ids(tokens[:MAX_LEN], vocab)\n",
    "    encoded_sequences.append(torch.tensor(token_ids, dtype=torch.long))\n",
    "\n",
    "# Padding\n",
    "padded_sequences = pad_sequence(encoded_sequences, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "labels_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "print(padded_sequences.shape)  # (num_samples, MAX_LEN)\n",
    "print(labels_tensor.shape)     # (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    3,  ...,  105,  113,   17],\n",
       "        [   5,    6,    7,  ...,   13,  231,   17],\n",
       "        [   5,    6,    7,  ...,   13,  231,   17],\n",
       "        ...,\n",
       "        [ 709,   16,   11,  ..., 6300,   17, 6539],\n",
       "        [ 709,   16,   11,  ...,   40,  393,   99],\n",
       "        [ 709,   16,   11,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el dataset con el que entrenaremos nuestra RNN. Primero crearemos un tensor combinando nuestras secuencias y las etiquetas correspondientes. Después lo dividiremos en conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(padded_sequences, labels_tensor)\n",
    "\n",
    "# División train/val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diseño del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CodeClassifierRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        _, (hn, _) = self.lstm(x)  # hn: (1, batch, hidden_dim)\n",
    "        return self.fc(hn.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CodeClassifierRNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=len(label_encoder.classes_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model = CodeClassifierRNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_report(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 2.1226 | Val Accuracy: 0.3632\n",
      "Epoch 2/100 | Train Loss: 1.8011 | Val Accuracy: 0.4129\n",
      "Epoch 3/100 | Train Loss: 1.9958 | Val Accuracy: 0.3284\n",
      "Epoch 4/100 | Train Loss: 1.7761 | Val Accuracy: 0.4129\n",
      "Epoch 5/100 | Train Loss: 1.6959 | Val Accuracy: 0.4080\n",
      "Epoch 6/100 | Train Loss: 1.4280 | Val Accuracy: 0.4677\n",
      "Epoch 7/100 | Train Loss: 1.4065 | Val Accuracy: 0.3980\n",
      "Epoch 8/100 | Train Loss: 1.4851 | Val Accuracy: 0.4478\n",
      "Epoch 9/100 | Train Loss: 1.3520 | Val Accuracy: 0.4876\n",
      "Epoch 10/100 | Train Loss: 1.5026 | Val Accuracy: 0.4726\n",
      "Epoch 11/100 | Train Loss: 1.5956 | Val Accuracy: 0.4677\n",
      "Epoch 12/100 | Train Loss: 1.1153 | Val Accuracy: 0.5672\n",
      "Epoch 13/100 | Train Loss: 0.9785 | Val Accuracy: 0.5672\n",
      "Epoch 14/100 | Train Loss: 1.0507 | Val Accuracy: 0.5323\n",
      "Epoch 15/100 | Train Loss: 1.0320 | Val Accuracy: 0.5473\n",
      "Epoch 16/100 | Train Loss: 1.3689 | Val Accuracy: 0.5075\n",
      "Epoch 17/100 | Train Loss: 1.4811 | Val Accuracy: 0.4975\n",
      "Epoch 18/100 | Train Loss: 1.1564 | Val Accuracy: 0.5423\n",
      "Epoch 19/100 | Train Loss: 1.0086 | Val Accuracy: 0.5771\n",
      "Epoch 20/100 | Train Loss: 0.9622 | Val Accuracy: 0.5721\n",
      "Epoch 21/100 | Train Loss: 0.9345 | Val Accuracy: 0.6070\n",
      "Epoch 22/100 | Train Loss: 0.8924 | Val Accuracy: 0.6169\n",
      "Epoch 23/100 | Train Loss: 0.7733 | Val Accuracy: 0.6617\n",
      "Epoch 24/100 | Train Loss: 0.7859 | Val Accuracy: 0.6617\n",
      "Epoch 25/100 | Train Loss: 0.7208 | Val Accuracy: 0.6667\n",
      "Epoch 26/100 | Train Loss: 0.6598 | Val Accuracy: 0.6816\n",
      "Epoch 27/100 | Train Loss: 0.7852 | Val Accuracy: 0.6816\n",
      "Epoch 28/100 | Train Loss: 0.9946 | Val Accuracy: 0.5970\n",
      "Epoch 29/100 | Train Loss: 0.9009 | Val Accuracy: 0.6269\n",
      "Epoch 30/100 | Train Loss: 0.8127 | Val Accuracy: 0.6567\n",
      "Epoch 31/100 | Train Loss: 1.1019 | Val Accuracy: 0.4975\n",
      "Epoch 32/100 | Train Loss: 1.2454 | Val Accuracy: 0.6318\n",
      "Epoch 33/100 | Train Loss: 1.2709 | Val Accuracy: 0.3881\n",
      "Epoch 34/100 | Train Loss: 1.3966 | Val Accuracy: 0.5771\n",
      "Epoch 35/100 | Train Loss: 0.9671 | Val Accuracy: 0.6567\n",
      "Epoch 36/100 | Train Loss: 0.8286 | Val Accuracy: 0.6070\n",
      "Epoch 37/100 | Train Loss: 0.7881 | Val Accuracy: 0.6716\n",
      "Epoch 38/100 | Train Loss: 0.7263 | Val Accuracy: 0.6965\n",
      "Epoch 39/100 | Train Loss: 0.6789 | Val Accuracy: 0.6866\n",
      "Epoch 40/100 | Train Loss: 0.7138 | Val Accuracy: 0.6965\n",
      "Epoch 41/100 | Train Loss: 0.7439 | Val Accuracy: 0.6866\n",
      "Epoch 42/100 | Train Loss: 0.6568 | Val Accuracy: 0.7015\n",
      "Epoch 43/100 | Train Loss: 0.6320 | Val Accuracy: 0.6866\n",
      "Epoch 44/100 | Train Loss: 0.6009 | Val Accuracy: 0.7015\n",
      "Epoch 45/100 | Train Loss: 0.5725 | Val Accuracy: 0.7065\n",
      "Epoch 46/100 | Train Loss: 0.5406 | Val Accuracy: 0.7512\n",
      "Epoch 47/100 | Train Loss: 0.5127 | Val Accuracy: 0.7313\n",
      "Epoch 48/100 | Train Loss: 0.5187 | Val Accuracy: 0.7562\n",
      "Epoch 49/100 | Train Loss: 0.5090 | Val Accuracy: 0.7313\n",
      "Epoch 50/100 | Train Loss: 0.6978 | Val Accuracy: 0.6368\n",
      "Epoch 51/100 | Train Loss: 1.3433 | Val Accuracy: 0.5075\n",
      "Epoch 52/100 | Train Loss: 1.0838 | Val Accuracy: 0.6219\n",
      "Epoch 53/100 | Train Loss: 0.9188 | Val Accuracy: 0.6318\n",
      "Epoch 54/100 | Train Loss: 0.8816 | Val Accuracy: 0.6567\n",
      "Epoch 55/100 | Train Loss: 0.8575 | Val Accuracy: 0.6219\n",
      "Epoch 56/100 | Train Loss: 0.7589 | Val Accuracy: 0.6567\n",
      "Epoch 57/100 | Train Loss: 0.6277 | Val Accuracy: 0.6567\n",
      "Epoch 58/100 | Train Loss: 0.5885 | Val Accuracy: 0.7413\n",
      "Epoch 59/100 | Train Loss: 0.5599 | Val Accuracy: 0.7015\n",
      "Epoch 60/100 | Train Loss: 0.6726 | Val Accuracy: 0.6119\n",
      "Epoch 61/100 | Train Loss: 0.7400 | Val Accuracy: 0.7065\n",
      "Epoch 62/100 | Train Loss: 0.6461 | Val Accuracy: 0.6866\n",
      "Epoch 63/100 | Train Loss: 0.6504 | Val Accuracy: 0.6816\n",
      "Epoch 64/100 | Train Loss: 0.5427 | Val Accuracy: 0.7264\n",
      "Epoch 65/100 | Train Loss: 1.0155 | Val Accuracy: 0.6219\n",
      "Epoch 66/100 | Train Loss: 0.7312 | Val Accuracy: 0.6766\n",
      "Epoch 67/100 | Train Loss: 0.6414 | Val Accuracy: 0.6766\n",
      "Epoch 68/100 | Train Loss: 0.6107 | Val Accuracy: 0.7065\n",
      "Epoch 69/100 | Train Loss: 0.6064 | Val Accuracy: 0.7562\n",
      "Epoch 70/100 | Train Loss: 0.5201 | Val Accuracy: 0.7164\n",
      "Epoch 71/100 | Train Loss: 0.4708 | Val Accuracy: 0.7512\n",
      "Epoch 72/100 | Train Loss: 0.5117 | Val Accuracy: 0.7463\n",
      "Epoch 73/100 | Train Loss: 0.4814 | Val Accuracy: 0.7711\n",
      "Epoch 74/100 | Train Loss: 0.5587 | Val Accuracy: 0.7512\n",
      "Epoch 75/100 | Train Loss: 0.4601 | Val Accuracy: 0.7960\n",
      "Epoch 76/100 | Train Loss: 0.4204 | Val Accuracy: 0.8109\n",
      "Epoch 77/100 | Train Loss: 0.3753 | Val Accuracy: 0.7811\n",
      "Epoch 78/100 | Train Loss: 0.3484 | Val Accuracy: 0.7313\n",
      "Epoch 79/100 | Train Loss: 0.4081 | Val Accuracy: 0.7313\n",
      "Epoch 80/100 | Train Loss: 0.3723 | Val Accuracy: 0.7910\n",
      "Epoch 81/100 | Train Loss: 0.5000 | Val Accuracy: 0.7363\n",
      "Epoch 82/100 | Train Loss: 0.5013 | Val Accuracy: 0.8060\n",
      "Epoch 83/100 | Train Loss: 0.3287 | Val Accuracy: 0.8408\n",
      "Epoch 84/100 | Train Loss: 0.3377 | Val Accuracy: 0.8607\n",
      "Epoch 85/100 | Train Loss: 0.2958 | Val Accuracy: 0.8706\n",
      "Epoch 86/100 | Train Loss: 0.2134 | Val Accuracy: 0.8905\n",
      "Epoch 87/100 | Train Loss: 0.1389 | Val Accuracy: 0.9055\n",
      "Epoch 88/100 | Train Loss: 0.0993 | Val Accuracy: 0.9154\n",
      "Epoch 89/100 | Train Loss: 0.1080 | Val Accuracy: 0.9104\n",
      "Epoch 90/100 | Train Loss: 0.1016 | Val Accuracy: 0.9055\n",
      "Epoch 91/100 | Train Loss: 0.0506 | Val Accuracy: 0.9303\n",
      "Epoch 92/100 | Train Loss: 0.0367 | Val Accuracy: 0.9502\n",
      "Epoch 93/100 | Train Loss: 0.0286 | Val Accuracy: 0.9353\n",
      "Epoch 94/100 | Train Loss: 0.0289 | Val Accuracy: 0.9453\n",
      "Epoch 95/100 | Train Loss: 0.0224 | Val Accuracy: 0.9403\n",
      "Epoch 96/100 | Train Loss: 0.0372 | Val Accuracy: 0.8856\n",
      "Epoch 97/100 | Train Loss: 0.0729 | Val Accuracy: 0.9353\n",
      "Epoch 98/100 | Train Loss: 0.0288 | Val Accuracy: 0.9303\n",
      "Epoch 99/100 | Train Loss: 0.0259 | Val Accuracy: 0.9453\n",
      "Epoch 100/100 | Train Loss: 0.0184 | Val Accuracy: 0.9353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   1.00      1.00      1.00         1\n",
      "           c       0.85      1.00      0.92        23\n",
      "         cpp       0.91      0.87      0.89        23\n",
      "          cs       1.00      0.71      0.83        17\n",
      "           d       0.93      0.93      0.93        14\n",
      "          hs       0.95      1.00      0.98        20\n",
      "        java       0.83      0.91      0.87        22\n",
      "          js       1.00      0.95      0.98        21\n",
      "         php       1.00      1.00      1.00        26\n",
      "          py       1.00      0.94      0.97        18\n",
      "          rs       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.94       201\n",
      "   macro avg       0.95      0.94      0.94       201\n",
      "weighted avg       0.94      0.94      0.93       201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=100)\n",
    "final_report(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
